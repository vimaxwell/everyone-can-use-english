import { t } from "i18next";

export const GPT_PROVIDERS: { [key: string]: any } = {
  enjoyai: {
    name: "EnjoyAI",
    models: [
      "gpt-4o",
      "gpt-4-turbo",
      "gpt-4-turbo-preview",
      "gpt-4-vision-preview",
      "gpt-4",
      "gpt-4-32k",
      "gpt-3.5-turbo",
      "gpt-3.5-turbo-16k",
      "gpt-3.5-turbo-instruct",
    ],
    configurable: [
      "model",
      "roleDefinition",
      "temperature",
      "numberOfChoices",
      "maxTokens",
      "frequencyPenalty",
      "presencePenalty",
      "historyBufferSize",
      "tts",
    ],
  },
  openai: {
    name: "OpenAI",
    description: t("youNeedToSetupApiKeyBeforeUsingOpenAI"),
    models: [
      "gpt-4o",
      "gpt-4-turbo",
      "gpt-4-turbo-preview",
      "gpt-4-vision-preview",
      "gpt-4",
      "gpt-4-32k",
      "gpt-3.5-turbo",
      "gpt-3.5-turbo-16k",
      "gpt-3.5-turbo-instruct",
    ],
    configurable: [
      "model",
      "baseUrl",
      "roleDefinition",
      "temperature",
      "numberOfChoices",
      "maxTokens",
      "frequencyPenalty",
      "presencePenalty",
      "historyBufferSize",
      "tts",
    ],
  },
  googleGenerativeAi: {
    name: "Google Generative AI",
    models: ["gemini-pro"],
    configurable: [
      "model",
      "roleDefinition",
      "temperature",
      "maxTokens",
      "historyBufferSize",
      "tts",
    ],
  },
  ollama: {
    name: "Ollama",
    description: t("ensureYouHaveOllamaRunningLocallyAndHasAtLeastOneModel"),
    baseUrl: "http://localhost:11434",
    models: [],
    configurable: [
      "model",
      "baseUrl",
      "roleDefinition",
      "temperature",
      "maxTokens",
      "historyBufferSize",
      "frequencyPenalty",
      "presencePenalty",
      "tts",
    ],
  },
};
